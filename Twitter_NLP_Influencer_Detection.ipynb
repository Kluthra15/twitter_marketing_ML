{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER ANALYSIS - MENTORSHIP PROJECT\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1. CONFIGURATION: ESTABLISHING CONNECTION TO THE API\n",
    "*Using Tweepy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "import configparser\n",
    "import requests     # For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os           # For dealing with json responses we receive from the API\n",
    "import json         # For displaying the data after\n",
    "import pandas as pd # For saving the response data in CSV format\n",
    "import csv          # For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata  #To add wait time between requests\n",
    "import time\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read configs\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "api_key             = config['twitter']['api_key']\n",
    "api_key_secret      = config['twitter']['api_key_secret']\n",
    "\n",
    "access_token        = config['twitter']['access_token']\n",
    "access_token_secret = config['twitter']['access_token_secret']\n",
    "\n",
    "bearer_token        = config['twitter']['bearer_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLuthra_\n"
     ]
    }
   ],
   "source": [
    "#Authenticate our account with the Twitter API\n",
    "auth    = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api     = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    \n",
    "# You can authenticate as your app with just your bearer token\n",
    "client  = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "# If the authentication was successful, this should print the\n",
    "# screen name / username of the account\n",
    "print(api.verify_credentials().screen_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2. DATA EXTRACTION & STORAGE\n",
    "####  2.1. Defining Data Model Schemas for Tweet & User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up SQLite database\n",
    "conn = sqlite3.connect('twitter_data.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop existing tables\n",
    "# conn.close()\n",
    "# c.execute('DROP TABLE IF EXISTS tweets')\n",
    "# c.execute('DROP TABLE IF EXISTS users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x21590372f80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create table for tweet data\n",
    "# c.execute('''CREATE TABLE IF NOT EXISTS tweets\n",
    "#              (tweet_id TEXT PRIMARY KEY,\n",
    "#               author_id TEXT,\n",
    "#               created_at TIMESTAMP,\n",
    "#               text TEXT,\n",
    "#               tweet_metrics JSON,\n",
    "#               entities JSON,\n",
    "#               context JSON,\n",
    "#               place_id JSON,\n",
    "#               FOREIGN KEY (author_id) REFERENCES users(author_id),\n",
    "#               FOREIGN KEY (place_id) REFERENCES users(place_id))''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in 'tweets' table: 0\n"
     ]
    }
   ],
   "source": [
    "# c.execute(\"SELECT COUNT(*) FROM tweets\")\n",
    "# row_count = c.fetchone()[0]\n",
    "# print(f\"Number of rows in 'tweets' table: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x21590372f80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create table for user data\n",
    "# c.execute('''CREATE TABLE IF NOT EXISTS users\n",
    "#              (author_id TEXT PRIMARY KEY,\n",
    "#               username TEXT,\n",
    "#               verified TEXT,\n",
    "#               bio TEXT,\n",
    "#               author_created TIMESTAMP,\n",
    "#               author_location TEXT,\n",
    "#               followers_count INTEGER,\n",
    "#               following_count INTEGER,\n",
    "#               tweet_count INTEGER,\n",
    "#               entities JSON,\n",
    "#               FOREIGN KEY (author_id) REFERENCES tweets(author_id))''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2. Defining a tweet fetching function using Tweepy\n",
    "\n",
    "**__Pagination:__** Pagination is a feature in Twitter API v2 endpoints that return more results than can be returned in a single response. When that happens, the data is returned in a series of 'pages'. Pagination refers to methods for programatically requesting all of the pages, in order to retrieve the entire result data set. Not all API endpoints support or require pagination, but it is often used when result sets are large.\n",
    "\n",
    "**Paginator** can be used to paginate for any Client methods that support pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(query, max_results):\n",
    "\n",
    "  expansions    = ['author_id','in_reply_to_user_id','geo.place_id','entities.mentions.username','referenced_tweets.id','referenced_tweets.id.author_id']\n",
    "  tweet_fields  = ['id','text','author_id','attachments','context_annotations','created_at','entities','lang','geo','public_metrics']\n",
    "  user_fields   = ['id','name','username','created_at','description','entities','location','public_metrics','verified']\n",
    "  place_fields  = ['full_name','id','country','country_code','geo','name','place_type']\n",
    "  try:\n",
    "    # call twitter api to fetch tweets\n",
    "    fetched_tweets = tweepy.Paginator(client.search_recent_tweets, query=query,\n",
    "      expansions        =expansions,\n",
    "      tweet_fields      =tweet_fields,\n",
    "      place_fields      =place_fields,\n",
    "      user_fields       =user_fields,   \n",
    "      max_results       =max_results\n",
    "    ).flatten()\n",
    "    \n",
    "    return fetched_tweets\n",
    "    \n",
    "\n",
    "  except Exception as e:\n",
    "    print(\"Error getting tweets\", e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2. Extracting Domains and Entities from the Twitter API\n",
    "*Annotations have been added to the Tweet object from all v2 endpoints that return a Tweet object. Tweet annotations offer a way to understand contextual information about the Tweet itself. Though 100% of Tweets are reviewed, due to the contents of Tweet text, only a portion are annotated.*\n",
    "\n",
    "##### **Tweet annotation types**\n",
    "**Entities** Entity annotations are programmatically defined entities that are nested within the entities field and are reflected as annotations in the payload. Each annotation has a confidence score and an indication of where in the Tweet text the entities were identified (start and end fields).\n",
    "\n",
    "The entity annotations can have the following types:\n",
    "\n",
    "1. Person - Barack Obama, Daniel, or George W. Bush\n",
    "2. Place - Detroit, Cali, or \"San Francisco, California\"\n",
    "3. Product - Mountain Dew, Mozilla Firefox\n",
    "4. Organization - Chicago White Sox, IBM\n",
    "5. Other - Diabetes, Super Bowl 50\n",
    "\n",
    "**Context annotations** are delivered as a context_annotations field in the payload. These annotations are inferred based on semantic analysis (keywords, hashtags, handles, etc) of the Tweet text and result in domain and/or entity labels. Context annotations can yield one or many domains. At present, weâ€™re using a list of 80+ domains reflected in the table below.  \n",
    "1. ID - 45: Brand Vertical\n",
    "2. ID - 46: Brand Category\n",
    "3. ID - 47: Brand\n",
    "4. ID - 48: Product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 DOMAIN-ENTITY QUERY CONSTRUCTION \n",
    "The *search_recent_tweets* function within the Twitter API has a query limit of 512 characters. To work around this, I have created a list of strings, less than 512 characters long, which contain the domain_id.entity_id search query broken up into chunks of 512 characters or less each which I will iterate through when making API requests to retrieve tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df = pd.read_csv('twitter-context-annotations/files/evergreen-context-entities-20220601.csv')\n",
    "import itertools\n",
    "def automate_domain_filter(df, start_id, end_id, chunk_size, domain_chunk_count):\n",
    "    chunks_list = []\n",
    "    for i in range(start_id, end_id+1):\n",
    "        context_list = []\n",
    "        mask = df['domains'].str.contains('^{}$'.format(i))\n",
    "        filtered_df = df[mask]\n",
    "        for index, row in filtered_df.iterrows():\n",
    "            domain_id = row['domains']\n",
    "            entity_id = row['entity_id']\n",
    "            entity_name = row['entity_name']   \n",
    "            # construct the query string\n",
    "            context = f'context:{domain_id}.{entity_id}'\n",
    "            context_list.append(context)\n",
    "            context_query = ' OR '.join(context_list)\n",
    "        code = context_query\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        counter = 0\n",
    "        while start < len(code) and counter < domain_chunk_count[i]:\n",
    "            end = start + chunk_size\n",
    "            if end >= len(code):\n",
    "                end = len(code)\n",
    "            end = code.rfind(\" OR \", start, end)\n",
    "            if end == -1:\n",
    "                end = start + chunk_size\n",
    "            chunk = code[start:end]\n",
    "            if chunk.startswith(\" OR \"):\n",
    "                chunk = chunk[4:]\n",
    "            chunks.append(chunk)\n",
    "            start = end\n",
    "            counter += 1\n",
    "        chunks_list.append(chunks)\n",
    "    return list(itertools.chain.from_iterable(chunks_list))\n",
    "\n",
    "chunk_size = 350\n",
    "domain_chunk_count = {45: 1, 46: 6, 47: 276, 48: 69}\n",
    "chunks_list = automate_domain_filter(domain_df, 45, 48, chunk_size, domain_chunk_count)\n",
    "# print(chunks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_df = pd.read_csv('twitter-context-annotations/files/evergreen-context-entities-20220601.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Defining Pre-Extraction Filtering \n",
    "- [ X ] Language restricted to English \n",
    "- [ X ] No Retweets or Quote Retweets Allowed\n",
    "- [ X ] Filtering for tweets within Domains 45 through 48 (*all entities*)\n",
    "- [ X ] Tweets must have mentions (*indicates presence of brand/sponsor*)\n",
    "- [ X ] Hashtag List consisting of indications that the tweet is being promoted or sponsored\n",
    "- [ ] Possible Entity Names which are irrelevant\n",
    "\n",
    "##### 2.2.3 Defining Post-Extraction Filtering \n",
    "  1. Accounts that have a high ratio of followers to following (e.g., following fewer than 100 accounts but having thousands of followers)\n",
    "  2. Number of Followers\n",
    "  1. Accounts that use a large number of hashtags in their tweets (e.g., more than 5 hashtags per tweet).\n",
    "  2. Accounts that use a lot of capital letters or exclamation points in their tweets.\n",
    "  3. Accounts that have a high percentage of tweets that contain links (e.g., more than 50% of tweets contain links).   \n",
    "  5. Using the Botometer API to extract a score for each user that indicates the probabibily of the account being a bot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rule(chunk, hash_include=True):\n",
    "    text_list       = '(#ad OR #sponsored OR #promoted OR \"Learn More\" OR \"Shop Now\")'\n",
    "    lang            = '(lang:en)'\n",
    "    rt              = '(-is:retweet) (-\"RT\")' \n",
    "    domain            = chunk\n",
    "    mention         = 'has:mentions'\n",
    "    if hash_include == True:\n",
    "        query           = text_list + ' ' + lang + ' ' + rt + ' ' + mention + ' ' + '(' + domain + ')'\n",
    "    else: \n",
    "        query           = lang + ' ' + rt + ' ' + mention + ' ' + '(' + domain + ')'\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tweepy\n",
    "\n",
    "processed_tweets = 0\n",
    "total_tweets = 0\n",
    "\n",
    "for chunk in chunks_list:\n",
    "    print(f'Chunk: {chunk}')\n",
    "    \n",
    "    query = filter_rule(chunk=chunk)\n",
    "    paginator = get_tweets(query=query, max_results=100)\n",
    "\n",
    "    if paginator is None:\n",
    "        print('Error: Paginator is None. Skipping chunk.')\n",
    "        continue\n",
    "    \n",
    "    for tweet in paginator:\n",
    "        try:\n",
    "            c.execute('''INSERT OR REPLACE INTO tweets \n",
    "                         (tweet_id, author_id, created_at, text, tweet_metrics, entities, context, place_id) \n",
    "                         VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',\n",
    "                      (tweet.id, tweet.author_id, tweet.created_at,\n",
    "                       tweet.text, json.dumps(tweet.public_metrics), \n",
    "                       json.dumps(tweet.entities), json.dumps(tweet.context_annotations),\n",
    "                       json.dumps(tweet.geo) if tweet.geo else None))\n",
    "            \n",
    "            processed_tweets += 1\n",
    "            print(f'Progress: {processed_tweets} tweets processed.')\n",
    "        \n",
    "        except tweepy.TweepError as e:\n",
    "            if e.response and e.response.status_code == 429:\n",
    "                print('Rate limit exceeded. Pausing for 15 minutes.')\n",
    "                print(f'Progress: {processed_tweets}/{total_tweets} tweets processed.')\n",
    "                time.sleep(10 * 60)  # Pause execution for 15 minutes (900 seconds)\n",
    "            else:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        except tweepy.TooManyRequests as e:\n",
    "            print('Rate limit exceeded. Pausing for 15 minutes.')\n",
    "            print(f'Progress: {processed_tweets}/{total_tweets} tweets processed.')\n",
    "            time.sleep(10 * 60)  # Pause execution for 15 minutes (900 seconds)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        total_tweets += 1\n",
    "        # time.sleep(10*60)  # Add a small delay between requests to avoid hitting rate limits\n",
    "    \n",
    "    print(f'Finished processing chunk: {chunk}')\n",
    "    print(f'Progress: {processed_tweets}/{total_tweets} tweets processed.')\n",
    "    conn.commit()  # Commit the changes to the database\n",
    "    time.sleep(60)  # Pause for 5 minutes between chunks to avoid hitting rate limits\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in 'tweets' table: 10897\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT COUNT(*) FROM tweets\")\n",
    "row_count = c.fetchone()[0]\n",
    "print(f\"Number of rows in 'tweets' table: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-open the connection to the database\n",
    "conn = sqlite3.connect('twitter_data.db')\n",
    "c = conn.cursor()\n",
    "# Get unique author IDs from the tweets table\n",
    "c.execute(\"SELECT DISTINCT author_id FROM tweets\")\n",
    "author_ids = [row[0] for row in c.fetchall()]\n",
    "\n",
    "user_data = []\n",
    "batch_size = 100\n",
    "n = 0\n",
    "# Iterate over batches of author IDs\n",
    "for i in range(0, len(author_ids), batch_size):      \n",
    "        # try:\n",
    "        users = client.get_users(ids=author_ids[i:i+batch_size], ##initially using api.lookup_users\n",
    "        user_fields=['id','name','username','created_at','description','entities','location','public_metrics','verified'])  \n",
    "          \n",
    "            # Insert the user data into the database\n",
    "        for user in users.data:\n",
    "                c.execute(\"DELETE FROM users WHERE author_id=?\", (user.id,))\n",
    "                c.execute('''INSERT INTO users (author_id, username, verified, bio, author_created, author_location, \n",
    "                followers_count, following_count, tweet_count, entities)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',\n",
    "                (user.id, user.username, user.verified, user.description, user.created_at,\n",
    "                user.location, user.public_metrics['followers_count'], user.public_metrics['following_count'],\n",
    "                user.public_metrics['tweet_count'], json.dumps(user.entities)))\n",
    "\n",
    "                 # Display the author information being stored in the database\n",
    "                print(f\"Stored author: {user.name} (@{user.username}), id={user.id}\")\n",
    "         \n",
    "        time.sleep(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af5e1ee4a7b9d36dcece4b58ca2b929e916944d8e9e9b48a2729c8b3b73a161a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
