{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER ANALYSIS - MENTORSHIP PROJECT\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1. CONFIGURATION: ESTABLISHING CONNECTION TO THE API\n",
    "*Using Tweepy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "import configparser\n",
    "import requests     # For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os           # For dealing with json responses we receive from the API\n",
    "import json         # For displaying the data after\n",
    "import pandas as pd # For saving the response data in CSV format\n",
    "import csv          # For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata  #To add wait time between requests\n",
    "import time\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read configs\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "api_key             = config['twitter']['api_key']\n",
    "api_key_secret      = config['twitter']['api_key_secret']\n",
    "\n",
    "access_token        = config['twitter']['access_token']\n",
    "access_token_secret = config['twitter']['access_token_secret']\n",
    "\n",
    "bearer_token        = config['twitter']['bearer_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLuthra_\n"
     ]
    }
   ],
   "source": [
    "#Authenticate our account with the Twitter API\n",
    "auth    = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api     = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    \n",
    "# You can authenticate as your app with just your bearer token\n",
    "client  = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "# If the authentication was successful, this should print the\n",
    "# screen name / username of the account\n",
    "print(api.verify_credentials().screen_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2. DATA EXTRACTION & STORAGE\n",
    "####  2.1. Defining Data Model Schemas for Tweet & User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up SQLite database\n",
    "conn = sqlite3.connect('twitter_data.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x22da7a02f80>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop existing tables\n",
    "c.execute('DROP TABLE IF EXISTS tweets')\n",
    "c.execute('DROP TABLE IF EXISTS users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x22da7a02f80>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table for tweet data\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS tweets\n",
    "             (tweet_id INTEGER PRIMARY KEY,\n",
    "              author_id INTEGER,\n",
    "              created_at TIMESTAMP,\n",
    "              text TEXT,\n",
    "              tweet_metrics JSON,\n",
    "              entities JSON,\n",
    "              context JSON,\n",
    "              place_id JSON,\n",
    "              FOREIGN KEY (author_id) REFERENCES users(author_id),\n",
    "              FOREIGN KEY (place_id) REFERENCES users(place_id))''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.execute(\"SELECT COUNT(*) FROM tweets\")\n",
    "# row_count = c.fetchone()[0]\n",
    "# print(f\"Number of rows in 'tweets' table: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x22da7a02f80>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table for user data\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS users\n",
    "             (author_id INTEGER PRIMARY KEY,\n",
    "              username TEXT,\n",
    "              verified TEXT,\n",
    "              bio TEXT,\n",
    "              author_created TIMESTAMP,\n",
    "              author_location TEXT,\n",
    "              followers_count INTEGER,\n",
    "              following_count INTEGER,\n",
    "              tweet_count INTEGER,\n",
    "              entities JSON,\n",
    "              FOREIGN KEY (author_id) REFERENCES tweets(author_id))''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2. Defining a tweet fetching function using Tweepy\n",
    "\n",
    "**__Pagination:__** Pagination is a feature in Twitter API v2 endpoints that return more results than can be returned in a single response. When that happens, the data is returned in a series of 'pages'. Pagination refers to methods for programatically requesting all of the pages, in order to retrieve the entire result data set. Not all API endpoints support or require pagination, but it is often used when result sets are large.\n",
    "\n",
    "**Paginator** can be used to paginate for any Client methods that support pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(query, max_results):\n",
    "\n",
    "  expansions    = ['author_id','in_reply_to_user_id','geo.place_id','entities.mentions.username','referenced_tweets.id','referenced_tweets.id.author_id']\n",
    "  tweet_fields  = ['id','text','author_id','attachments','context_annotations','created_at','entities','lang','geo','public_metrics']\n",
    "  user_fields   = ['id','name','username','created_at','description','entities','location','public_metrics','verified']\n",
    "  place_fields  = ['full_name','id','country','country_code','geo','name','place_type']\n",
    "  try:\n",
    "    # call twitter api to fetch tweets\n",
    "    fetched_tweets = tweepy.Paginator(client.search_recent_tweets, query=query,\n",
    "      expansions        =expansions,\n",
    "      tweet_fields      =tweet_fields,\n",
    "      place_fields      =place_fields,\n",
    "      user_fields       =user_fields,   \n",
    "      max_results       =max_results\n",
    "    ).flatten()\n",
    "    \n",
    "    return fetched_tweets\n",
    "    \n",
    "\n",
    "  except Exception as e:\n",
    "    print(\"Error getting tweets\", e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2. Extracting Domains and Entities from the Twitter API\n",
    "*Annotations have been added to the Tweet object from all v2 endpoints that return a Tweet object. Tweet annotations offer a way to understand contextual information about the Tweet itself. Though 100% of Tweets are reviewed, due to the contents of Tweet text, only a portion are annotated.*\n",
    "\n",
    "##### **Tweet annotation types**\n",
    "**Entities** Entity annotations are programmatically defined entities that are nested within the entities field and are reflected as annotations in the payload. Each annotation has a confidence score and an indication of where in the Tweet text the entities were identified (start and end fields).\n",
    "\n",
    "The entity annotations can have the following types:\n",
    "\n",
    "1. Person - Barack Obama, Daniel, or George W. Bush\n",
    "2. Place - Detroit, Cali, or \"San Francisco, California\"\n",
    "3. Product - Mountain Dew, Mozilla Firefox\n",
    "4. Organization - Chicago White Sox, IBM\n",
    "5. Other - Diabetes, Super Bowl 50\n",
    "\n",
    "**Context annotations** are delivered as a context_annotations field in the payload. These annotations are inferred based on semantic analysis (keywords, hashtags, handles, etc) of the Tweet text and result in domain and/or entity labels. Context annotations can yield one or many domains. At present, weâ€™re using a list of 80+ domains reflected in the table below.  \n",
    "1. ID - 45: Brand Vertical\n",
    "2. ID - 46: Brand Category\n",
    "3. ID - 47: Brand\n",
    "4. ID - 48: Product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 DOMAIN-ENTITY QUERY CONSTRUCTION \n",
    "The *search_recent_tweets* function within the Twitter API has a query limit of 512 characters. To work around this, I have created a list of strings, less than 512 characters long, which contain the domain_id.entity_id search query broken up into chunks of 512 characters or less each which I will iterate through when making API requests to retrieve tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df = pd.read_csv('twitter-context-annotations/files/evergreen-context-entities-20220601.csv')\n",
    "import itertools\n",
    "def automate_domain_filter(df, start_id, end_id, chunk_size, domain_chunk_count):\n",
    "    chunks_list = []\n",
    "    for i in range(start_id, end_id+1):\n",
    "        context_list = []\n",
    "        mask = df['domains'].str.contains('^{}$'.format(i))\n",
    "        filtered_df = df[mask]\n",
    "        for index, row in filtered_df.iterrows():\n",
    "            domain_id = row['domains']\n",
    "            entity_id = row['entity_id']\n",
    "            entity_name = row['entity_name']   \n",
    "            # construct the query string\n",
    "            context = f'context:{domain_id}.{entity_id}'\n",
    "            context_list.append(context)\n",
    "            context_query = ' OR '.join(context_list)\n",
    "        code = context_query\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        counter = 0\n",
    "        while start < len(code) and counter < domain_chunk_count[i]:\n",
    "            end = start + chunk_size\n",
    "            if end >= len(code):\n",
    "                end = len(code)\n",
    "            end = code.rfind(\" OR \", start, end)\n",
    "            if end == -1:\n",
    "                end = start + chunk_size\n",
    "            chunk = code[start:end]\n",
    "            if chunk.startswith(\" OR \"):\n",
    "                chunk = chunk[4:]\n",
    "            chunks.append(chunk)\n",
    "            start = end\n",
    "            counter += 1\n",
    "        chunks_list.append(chunks)\n",
    "    return list(itertools.chain.from_iterable(chunks_list))\n",
    "\n",
    "chunk_size = 350\n",
    "domain_chunk_count = {45: 1, 46: 6, 47: 276, 48: 69}\n",
    "chunks_list = automate_domain_filter(domain_df, 45, 48, chunk_size, domain_chunk_count)\n",
    "# print(chunks_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Defining Pre-Extraction Filtering \n",
    "- [ X ] Language restricted to English \n",
    "- [ X ] No Retweets or Quote Retweets Allowed\n",
    "- [ X ] Filtering for tweets within Domains 45 through 48 (*all entities*)\n",
    "- [ X ] Tweets must have mentions (*indicates presence of brand/sponsor*)\n",
    "- [ X ] Hashtag List consisting of indications that the tweet is being promoted or sponsored\n",
    "- [ ] Possible Entity Names which are irrelevant\n",
    "\n",
    "##### 2.2.3 Defining Post-Extraction Filtering \n",
    "  1. Accounts that have a high ratio of followers to following (e.g., following fewer than 100 accounts but having thousands of followers)\n",
    "  2. Number of Followers\n",
    "  1. Accounts that use a large number of hashtags in their tweets (e.g., more than 5 hashtags per tweet).\n",
    "  2. Accounts that use a lot of capital letters or exclamation points in their tweets.\n",
    "  3. Accounts that have a high percentage of tweets that contain links (e.g., more than 50% of tweets contain links).   \n",
    "  5. Using the Botometer API to extract a score for each user that indicates the probabibily of the account being a bot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rule(chunk, hash_include=True):\n",
    "    text_list       = '(#ad OR #sponsored OR #promoted OR \"Learn More\" OR \"Shop Now\")'\n",
    "    lang            = '(lang:en)'\n",
    "    rt              = '(-is:retweet) (-\"RT\")' \n",
    "    domain            = chunk\n",
    "    mention         = 'has:mentions'\n",
    "    if hash_include == True:\n",
    "        query           = text_list + ' ' + lang + ' ' + rt + ' ' + mention + ' ' + '(' + domain + ')'\n",
    "    else: \n",
    "        query           = lang + ' ' + rt + ' ' + mention + ' ' + '(' + domain + ')'\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "processed_tweets = 0\n",
    "total_tweets = 0\n",
    "\n",
    "tweet_data = []  # List to store tweet data\n",
    "\n",
    "for chunk in chunks_list:\n",
    "    print(f'Chunk: {chunk}')\n",
    "    \n",
    "    query = filter_rule(chunk=chunk)\n",
    "    paginator = get_tweets(query=query, max_results=100)\n",
    "\n",
    "    if paginator is None:\n",
    "        print('Error: Paginator is None. Skipping chunk.')\n",
    "        continue\n",
    "    \n",
    "    for tweet in paginator:\n",
    "        tweet_info = {\n",
    "                'tweet_id': tweet.id,\n",
    "                'author_id': tweet.author_id,\n",
    "                'created_at': tweet.created_at,\n",
    "                'text': tweet.text,\n",
    "                'tweet_metrics': json.dumps(tweet.public_metrics),\n",
    "                'entities': json.dumps(tweet.entities),\n",
    "                'context': json.dumps(tweet.context_annotations),\n",
    "                'place_id': json.dumps(tweet.geo) if tweet.geo else None\n",
    "            }\n",
    "            \n",
    "        tweet_data.append(tweet_info)\n",
    "        processed_tweets += 1\n",
    "    \n",
    "    \n",
    "    print(f'Finished processing chunk: {chunk}')\n",
    "    print(f'Progress: {processed_tweets} tweets processed.')\n",
    "    time.sleep(3)  # Pause for 5 minutes between chunks to avoid hitting rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22524, 8)\n",
      "Number of distinct tweet IDs: 11032\n"
     ]
    }
   ],
   "source": [
    "# Convert tweet data to a DataFrame\n",
    "df = pd.DataFrame(tweet_data)\n",
    "print(df.shape)\n",
    "\n",
    "num_distinct_tweets = df['tweet_id'].nunique()\n",
    "print(f\"Number of distinct tweet IDs: {num_distinct_tweets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11032, 8)\n"
     ]
    }
   ],
   "source": [
    "# Convert tweet_data list to a DataFrame\n",
    "df = pd.DataFrame(tweet_data)\n",
    "\n",
    "# Sort the DataFrame by 'tweet_id' in descending order\n",
    "df.sort_values('tweet_id', ascending=True, inplace=True)\n",
    "\n",
    "# Drop duplicate rows based on 'tweet_id' column, keeping the last occurrence\n",
    "dedup_df = df.drop_duplicates(subset='tweet_id', keep='last', inplace=False).reset_index(drop=True, inplace=False)\n",
    "\n",
    "print(dedup_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tweet_id                       int64\n",
      "author_id                      int64\n",
      "created_at       datetime64[ns, UTC]\n",
      "text                          object\n",
      "tweet_metrics                 object\n",
      "entities                      object\n",
      "context                       object\n",
      "place_id                      object\n",
      "dtype: object\n",
      "tweet_id - INTEGER\n",
      "author_id - INTEGER\n",
      "created_at - TIMESTAMP\n",
      "text - TEXT\n",
      "tweet_metrics - JSON\n",
      "entities - JSON\n",
      "context - JSON\n",
      "place_id - JSON\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(dedup_df.dtypes)\n",
    "\n",
    "c.execute(\"PRAGMA table_info(tweets)\")\n",
    "columns = c.fetchall()\n",
    "\n",
    "for column in columns:\n",
    "    print(column[1], \"-\", column[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = 0\n",
    "\n",
    "for index, tweet in dedup_df.iterrows():\n",
    "    try:\n",
    "        created_at = tweet['created_at'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        # Check if tweet with the same tweet_id already exists in the database\n",
    "        c.execute('SELECT tweet_id FROM tweets WHERE tweet_id=?', (tweet['tweet_id'],))\n",
    "        existing_tweet_id = c.fetchone()\n",
    "\n",
    "        if existing_tweet_id is None:\n",
    "            # Tweet doesn't exist in the database, insert it\n",
    "            c.execute('''INSERT INTO tweets \n",
    "                         (tweet_id, author_id, created_at, text, tweet_metrics, entities, context, place_id) \n",
    "                         VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',\n",
    "                      (tweet['tweet_id'], tweet['author_id'], created_at, tweet['text'],\n",
    "                       tweet['tweet_metrics'], tweet['entities'], tweet['context'], tweet['place_id']))\n",
    "            print(f\"New Tweet Appended\")\n",
    "        else:\n",
    "            # Tweet already exists, update tweet_metrics\n",
    "            c.execute('''UPDATE tweets \n",
    "                         SET tweet_metrics = ? \n",
    "                         WHERE tweet_id = ?''',\n",
    "                      (tweet['tweet_metrics'], tweet['tweet_id']))\n",
    "            print(f\"Tweet Already Exists, Updating Tweet Metrics\")\n",
    "        processed_tweets += 1\n",
    "        \n",
    "        print(f'Progress: {processed_tweets} tweets processed.')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting row: {tweet}\")\n",
    "        print(f\"Error message: {e}\")\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in 'tweets' table: 11032\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT COUNT(DISTINCT tweet_id) FROM tweets\")\n",
    "row_count = c.fetchone()[0]\n",
    "print(f\"Number of rows in 'tweets' table: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique author IDs from the tweets table\n",
    "c.execute(\"SELECT DISTINCT author_id FROM tweets\")\n",
    "author_ids = [row[0] for row in c.fetchall()]\n",
    "\n",
    "user_data = []\n",
    "batch_size = 100\n",
    "n = 0\n",
    "\n",
    "# Iterate over batches of author IDs\n",
    "for i in range(0, len(author_ids), batch_size):      \n",
    "    # try:\n",
    "    users = client.get_users(ids=author_ids[i:i+batch_size], user_fields=['id', 'name', 'username', 'created_at', 'description', 'entities', 'location', 'public_metrics', 'verified'])\n",
    "\n",
    "    # Insert or update the user data in the database\n",
    "    for user in users.data:\n",
    "        # Check if author already exists in the database\n",
    "        c.execute(\"SELECT author_id FROM users WHERE author_id=?\", (user.id,))\n",
    "        existing_author_id = c.fetchone()\n",
    "\n",
    "        if existing_author_id is None:\n",
    "            # Author doesn't exist in the database, insert a new row\n",
    "            author_created = user.created_at.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            c.execute('''INSERT INTO users (author_id, username, verified, bio, author_created, author_location, \n",
    "                         followers_count, following_count, tweet_count, entities)\n",
    "                         VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',\n",
    "                      (user.id, user.username, user.verified, user.description, author_created,\n",
    "                       user.location, user.public_metrics['followers_count'], user.public_metrics['following_count'],\n",
    "                       user.public_metrics['tweet_count'], json.dumps(user.entities)))\n",
    "            print(f\"Stored author: {user.name} (@{user.username}), id={user.id}\")\n",
    "        else:\n",
    "            # Author already exists in the database, update the existing row\n",
    "            author_created = user.created_at.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            c.execute('''UPDATE users SET username=?, verified=?, bio=?, author_created=?, author_location=?, \n",
    "                         followers_count=?, following_count=?, tweet_count=?, entities=?\n",
    "                         WHERE author_id=?''',\n",
    "                      (user.username, user.verified, user.description, author_created,\n",
    "                       user.location, user.public_metrics['followers_count'], user.public_metrics['following_count'],\n",
    "                       user.public_metrics['tweet_count'], json.dumps(user.entities), user.id))\n",
    "            print(f\"Updated author: {user.name} (@{user.username}), id={user.id}\")\n",
    "\n",
    "    time.sleep(16)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af5e1ee4a7b9d36dcece4b58ca2b929e916944d8e9e9b48a2729c8b3b73a161a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
