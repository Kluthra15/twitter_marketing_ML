{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data from the SQLite database\n",
    "query = \"SELECT * FROM tweets\"\n",
    "tweets_df = pd.read_sql_query(query, conn, parse_dates=['created_at'])\n",
    "print(tweets_df.dtypes)\n",
    "\n",
    "\n",
    "# Retrieve the data from the SQLite database\n",
    "query = \"SELECT * FROM users\"\n",
    "users_df = pd.read_sql_query(query, conn, parse_dates=['author_created'])\n",
    "print(users_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(df):\n",
    "    \"\"\"\n",
    "    Aggregate data based on domain ID from the provided tweets DataFrame.\n",
    "\n",
    "    Args:\n",
    "        tweets_df (pd.DataFrame): DataFrame containing tweet data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with aggregated data based on domain ID.\n",
    "    \"\"\"\n",
    "    subset_df = df[['tweet_id', 'author_id', 'tweet_metrics', 'context', 'entities']]\n",
    "\n",
    "    subset_context_items = []\n",
    "    for index, row in subset_df.iterrows():\n",
    "        context_list = json.loads(row['context'])\n",
    "        tweet_id     = row['tweet_id']\n",
    "        author_id    = row['author_id']\n",
    "        for item in context_list:\n",
    "            domain_id   = item['domain']['id']\n",
    "            domain_name = item['domain']['name']\n",
    "            entity_id   = item['entity']['id']\n",
    "            entity_name = item['entity']['name']\n",
    "            data = {'tweet_id': tweet_id, 'author_id': author_id, 'domain_id':domain_id, 'domain_name': domain_name, 'entity_id':entity_id, 'entity_name': entity_name}\n",
    "            subset_context_items.append(data)\n",
    "\n",
    "    subset_context_item_df = pd.DataFrame(subset_context_items)\n",
    "    return subset_context_item_df\n",
    "\n",
    "subset_context_item_df = aggregate_data(df=tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_counts(data_frame, group_by_cols, aggregate_col, top_n):\n",
    "    \"\"\"\n",
    "    Calculate the top counts based on the given DataFrame, group by columns, aggregate column, and number of top counts to retrieve.\n",
    "    \n",
    "    Parameters:\n",
    "        data_frame (pandas.DataFrame): DataFrame containing the data to perform calculations on.\n",
    "        group_by_cols (list): List of columns to group by.\n",
    "        aggregate_col (str): Column to perform aggregation on.\n",
    "        top_n (int): Number of top counts to retrieve.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Top counts, sorted in descending order.\n",
    "    \"\"\"\n",
    "    subset_agg = data_frame.groupby(by=group_by_cols).agg({aggregate_col: pd.Series.nunique}).reset_index().sort_values(by=aggregate_col, ascending=False)\n",
    "    top_counts = subset_agg.head(top_n)\n",
    "    \n",
    "    return top_counts\n",
    "\n",
    "top_20_domain_counts = calculate_top_counts(subset_context_item_df, [\"domain_name\"], \"tweet_id\", 20)\n",
    "top_50_entity_counts = calculate_top_counts(subset_context_item_df, [\"entity_name\"], \"tweet_id\", 50)\n",
    "top_25_user_counts = calculate_top_counts(subset_context_item_df, [\"author_id\"], \"tweet_id\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, figsize = (25, 35))\n",
    "fig.suptitle('Volume of Tweets by Domain & Entity (Context Annotations)')\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(ax=axs[0], x=top_20_domain_counts['domain_name'], y=top_20_domain_counts['tweet_id'], color=\"b\")\n",
    "sns.barplot(ax=axs[1], x=top_50_entity_counts['tweet_id'], y=top_50_entity_counts['entity_name'], color=\"b\", orient='h')\n",
    "sns.barplot(ax=axs[2], x=top_25_user_counts['tweet_id'], y=top_25_user_counts['author_id'], color=\"b\", orient='h')\n",
    "\n",
    "\n",
    "# Rotate the x-axis labels\n",
    "axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=60)\n",
    "axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=60)\n",
    "axs[2].set_xticklabels(axs[2].get_xticklabels(), rotation=60)\n",
    "\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing to avoid label overlap\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the buckets for follower and following counts\n",
    "buckets = [0, 100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000, 10000000]\n",
    "\n",
    "# Count the number of users in each bucket\n",
    "follower_counts = pd.cut(users_df['followers_count'], buckets).value_counts().sort_index()\n",
    "following_counts = pd.cut(users_df['following_count'], buckets).value_counts().sort_index()\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 18))\n",
    "\n",
    "# Plot follower count distribution\n",
    "axs[0].bar(range(len(follower_counts)), follower_counts, width=0.4, align='center', alpha=0.5, color='blue')\n",
    "axs[0].set_xlabel('Bucket')\n",
    "axs[0].set_ylabel('User Count')\n",
    "axs[0].set_title('Distribution of Follower Counts')\n",
    "axs[0].set_xticks(range(len(follower_counts)))\n",
    "axs[0].set_xticklabels(follower_counts.index, rotation=45)\n",
    "\n",
    "# Plot following count distribution\n",
    "axs[1].bar(range(len(following_counts)), following_counts, width=0.4, align='center', alpha=0.5, color='red')\n",
    "axs[1].set_xlabel('Bucket')\n",
    "axs[1].set_ylabel('User Count')\n",
    "axs[1].set_title('Distribution of Following Counts')\n",
    "axs[1].set_xticks(range(len(following_counts)))\n",
    "axs[1].set_xticklabels(following_counts.index, rotation=45)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'verified' column to integer type\n",
    "users_df['verified'] = users_df['verified'].astype(int)\n",
    "\n",
    "# Filter verified users\n",
    "verified_users = users_df[users_df['verified'] == 1]\n",
    "non_verified_users = users_df[users_df['verified'] == 0]\n",
    "\n",
    "# Sort the verified users based on followers count in descending order\n",
    "top_verified_followed_users = verified_users.sort_values('followers_count', ascending=False).head(25)\n",
    "\n",
    "# Sort the verified users based on tweet count in descending order\n",
    "top_verified_tweet_users = verified_users.sort_values('tweet_count', ascending=False).head(25)\n",
    "\n",
    "# Sort the non-verified users based on followers count in descending order\n",
    "top_non_verified_followed_users = non_verified_users.sort_values('followers_count', ascending=False).head(25)\n",
    "\n",
    "# Sort the non-verified users based on tweet count in descending order\n",
    "top_non_verified_tweet_users = non_verified_users.sort_values('tweet_count', ascending=False).head(25)\n",
    "\n",
    "# Create a figure with a 2x2 grid of subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Plot the top followed users (verified)\n",
    "axs[0, 0].barh(top_verified_followed_users['username'], top_verified_followed_users['followers_count'], color='blue')\n",
    "axs[0, 0].set_xlabel('Followers Count')\n",
    "axs[0, 0].set_ylabel('Username')\n",
    "axs[0, 0].set_title('Top 25 Most Followed Verified Users')\n",
    "axs[0, 0].invert_yaxis()\n",
    "\n",
    "# Plot the top tweet users (verified)\n",
    "axs[0, 1].barh(top_verified_tweet_users['username'], top_verified_tweet_users['tweet_count'], color='green')\n",
    "axs[0, 1].set_xlabel('Tweet Count')\n",
    "axs[0, 1].set_ylabel('Username')\n",
    "axs[0, 1].set_title('Top 25 Verified Users with Highest Tweet Counts')\n",
    "axs[0, 1].invert_yaxis()\n",
    "\n",
    "# Plot the top followed users (non-verified)\n",
    "axs[1, 0].barh(top_non_verified_followed_users['username'], top_non_verified_followed_users['followers_count'], color='blue')\n",
    "axs[1, 0].set_xlabel('Followers Count')\n",
    "axs[1, 0].set_ylabel('Username')\n",
    "axs[1, 0].set_title('Top 25 Most Followed Non-Verified Users')\n",
    "axs[1, 0].invert_yaxis()\n",
    "\n",
    "# Plot the top tweet users (non-verified)\n",
    "axs[1, 1].barh(top_non_verified_tweet_users['username'], top_non_verified_tweet_users['tweet_count'], color='green')\n",
    "axs[1, 1].set_xlabel('Tweet Count')\n",
    "axs[1, 1].set_ylabel('Username')\n",
    "axs[1, 1].set_title('Top 25 Non-Verified Users with Highest Tweet Counts')\n",
    "axs[1, 1].invert_yaxis()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "The following code will remove emoticons, hyperlinks, whitepsaces such as new lines and indentations.\n",
    "It will also tokenize text into words, remove slang words, and visualize the text pre-processed data to show patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnicode(text):\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
    "    return text\n",
    "\n",
    "def replaceURL(text):\n",
    "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEmoticons(text):\n",
    "    \"\"\" Removes emoticons from text \"\"\"\n",
    "    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
    "    return text\n",
    "\n",
    "def countEmoticons(text):\n",
    "    \"\"\" Input: a text, Output: how many emoticons \"\"\"\n",
    "    return len(re.findall(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "\"\"\" Tokenizes a text to its words, removes and replaces some of them \"\"\"    \n",
    "finalTokens = [] # all tokens\n",
    "stoplist = stopwords.words('english')\n",
    "my_stopwords = \"multiexclamation multiquestion multistop url atuser st rd nd th am pm\" # my extra stopwords\n",
    "stoplist = stoplist + my_stopwords.split()\n",
    "allowedWordTypes = [\"J\",\"R\",\"V\",\"N\"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging\n",
    "\n",
    "# Create a DataFrame to store the preprocessed text and tweet ID\n",
    "preprocessed_data = pd.DataFrame(columns=['tweet_id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, wordCountBefore, tweet_id):\n",
    "    totalAdjectives = 0\n",
    "    totalAdverbs = 0\n",
    "    totalVerbs = 0\n",
    "    onlyOneSentenceTokens = []  # tokens of one sentence each time\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)  # Technique 7: remove punctuation\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    tagged = nltk.pos_tag(tokens)  # Technique 13: part of speech tagging\n",
    "    for w in tagged:\n",
    "        if w[1][0] in allowedWordTypes and w[0] not in stoplist:\n",
    "            onlyOneSentenceTokens.append(w[0])\n",
    "            finalTokens.append(w[0])\n",
    "\n",
    "    onlyOneSentence = \" \".join(onlyOneSentenceTokens)\n",
    "\n",
    "    # Store the preprocessed text and tweet ID in the DataFrame\n",
    "    preprocessed_data.loc[len(preprocessed_data)] = [tweet_id, onlyOneSentence]\n",
    "\n",
    "    return finalTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Retrieve all tweets from the database\n",
    "query = \"SELECT DISTINCT * FROM tweets\"\n",
    "df_tweets = pd.read_sql_query(query, conn)\n",
    "#Tweet_id is column 0, text is column 3\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "totalSentences = 0\n",
    "totalEmoticons = 0\n",
    "totalSlangs = 0\n",
    "totalSlangsFound = []\n",
    "totalElongated = 0\n",
    "totalMultiExclamationMarks = 0\n",
    "totalMultiQuestionMarks = 0\n",
    "totalMultiStopMarks = 0\n",
    "totalAllCaps = 0\n",
    "\n",
    "# Iterate over each row in df_tweets\n",
    "for index, row in df_tweets.iterrows():\n",
    "    totalSentences += 1\n",
    "    feat = []\n",
    "\n",
    "    tweet_id = row['tweet_id']\n",
    "    text     = removeUnicode(row['text'])  # Technique 0\n",
    "    text     = replaceURL(row['text'])\n",
    "\n",
    "    wordCountBefore = len(re.findall(r'\\w+', text))  # word count of one sentence before preprocess\n",
    "\n",
    "    emoticons = countEmoticons(text)  # how many emoticons in this sentence\n",
    "    totalEmoticons += emoticons\n",
    "\n",
    "    text = removeEmoticons(text)  # removes emoticons from text\n",
    "    tokens = tokenize(text, wordCountBefore, tweet_id)  \n",
    "\n",
    "    # print(\"Processed tweet:\", tweet_id)  # Print the tweet ID after processing\n",
    "\n",
    "# View the resulting preprocessed data\n",
    "print(preprocessed_data.head())\n",
    "    \n",
    "print(\"Total sentences: \",                          totalSentences,\"\\n\")\n",
    "print(\"Total Words before preprocess: \",            len(re.findall(r'\\w+', ' '.join(df_tweets['text']))))\n",
    "print(\"Total Distinct Tokens before preprocess: \",  len(set(re.findall(r'\\w+', ' '.join(df_tweets['text'])))))\n",
    "print(\"Average word/sentence before preprocess: \",  len(re.findall(r'\\w+', ' '.join(df_tweets['text']))) / totalSentences, \"\\n\")\n",
    "print(\"Total Words after preprocess: \",             len(tokens))\n",
    "print(\"Total Distinct Tokens after preprocess: \",   len(set(tokens)))\n",
    "print(\"Average word/sentence after preprocess: \",   len(tokens)/totalSentences,\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Total run time: \",                           time() - t0,\" seconds\\n\")\n",
    "\n",
    "print(\"Total emoticons: \",                          totalEmoticons,\"\\n\")\n",
    "print(\"Total slangs: \",                             totalSlangs,\"\\n\")\n",
    "# commonSlangs = nltk.FreqDist(totalSlangsFound)\n",
    "# for (word, count) in commonSlangs.most_common(20): # most common slangs across all texts\n",
    "#     print(word,\"\\t\",count)\n",
    "# commonSlangs.plot(20, cumulative=False) # plot most common slangs\n",
    "\n",
    "print(\"Total elongated words: \",                    totalElongated,\"\\n\")\n",
    "print(\"Total multi exclamation marks: \",            totalMultiExclamationMarks)\n",
    "print(\"Total multi question marks: \",               totalMultiQuestionMarks)\n",
    "print(\"Total multi stop marks: \",                   totalMultiStopMarks,\"\\n\")\n",
    "print(\"Total all capitalized words: \",              totalAllCaps,\"\\n\")\n",
    "\n",
    "#print(tokens)\n",
    "commonWords = nltk.FreqDist(tokens)\n",
    "print(\"Most common words \")\n",
    "print(\"Word\\tCount\")\n",
    "for (word, count) in commonWords.most_common(100): # most common words across all texts\n",
    "    print(word,\"\\t\",count)\n",
    "\n",
    "# Create a larger plot with adjusted dimensions\n",
    "plt.figure(figsize=(20, 8))  # Adjust the width and height as needed\n",
    "commonWords.plot(100, cumulative=False) # plot most common words\n",
    "\n",
    "\n",
    "bgm = nltk.collocations.BigramAssocMeasures()\n",
    "tgm = nltk.collocations.TrigramAssocMeasures()\n",
    "bgm_finder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "tgm_finder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n",
    "bgm_finder.apply_freq_filter(5) # bigrams that occur at least 5 times\n",
    "print(\"Most common collocations (bigrams)\")\n",
    "print(bgm_finder.nbest(bgm.pmi, 50)) # top 50 bigram collocations\n",
    "tgm_finder.apply_freq_filter(5) # trigrams that occur at least 5 times\n",
    "print(\"Most common collocations (trigrams)\")\n",
    "print(tgm_finder.nbest(tgm.pmi, 20)) # top 20 trigrams collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the preprocessed_data DataFrame with df_tweets based on tweet_id\n",
    "df_tweets_preprocessed = df_tweets.merge(preprocessed_data, on='tweet_id', how='left', suffixes=('_original', '_preprocessed'))\n",
    "\n",
    "# Deduplicate the merged DataFrame based on the 'tweet_id' column\n",
    "df_tweets_preprocessed = df_tweets_preprocessed.drop_duplicates(subset=['tweet_id', 'text_preprocessed'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
