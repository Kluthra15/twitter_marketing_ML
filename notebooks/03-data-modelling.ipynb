{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "import configparser\n",
    "import requests     # For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os           # For dealing with json responses we receive from the API\n",
    "import json         # For displaying the data after\n",
    "import pandas as pd # For saving the response data in CSV format\n",
    "import csv          # For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata  #To add wait time between requests\n",
    "import time\n",
    "import sqlite3\n",
    "import re\n",
    "import twitter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance,PartOfSpeech\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "import numpy as np\n",
    "from twitter import *\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_preprocessed = pd.read_pickle('../data/df_tweets_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Parts in BERT TOPIC MODELLING: \n",
    "1. Embedding Model \n",
    " 2. Dimensionality Reduction \n",
    " 3. Clustering\n",
    " 4. Vectorizer\n",
    " 5. TF-IDF\n",
    " 6. Fine Tune Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMBEDDING MODEL\n",
    "BERTopic starts with transforming our input documents into numerical representations. Although there are many ways this can be achieved, we typically use sentence-transformers (\"all-MiniLM-L6-v2\") as it is quite capable of capturing the semantic similarity between documents.\n",
    "\n",
    "However, there is not one perfect embedding model and you might want to be using something entirely different for your use case. \n",
    "This modularity allows us not only to choose any embedding model to convert our documents into numerical representations, we can use essentially any data to perform our clustering. When new state-of-the-art pre-trained embedding models are released, BERTopic will be able to use them. As a result, BERTopic grows with any new models being released.\n",
    "\n",
    "#### UMAP & its Hyperparameters\n",
    "UMAP is a technique used for dimensionality reduction. In BERTopic, it is used to reduce the dimensionality of document embedding into something easier to use with HDBSCAN to create good clusters.\n",
    "Why this particular clustering model over others? this is because it automatically identifies the number of clusters as opposed to k-means for example which requires a trial and error test to figure out the right number. HDBSCAN does it based on a density based method. \n",
    "1. **n_neighbors**: number of neighboring sample points used when making the manifold approximation. Increasing this value often results in larger clusters being created.\n",
    "2. **n_components**:  refers to the dimensionality of the embeddings after reducing them. This is set as a default to 5 to reduce dimensionality as much as possible whilst trying to maximize the information kept in the resulting embeddings. Although lowering or increasing this value influences the quality of embeddings, its effect is largest on the performance of HDBSCAN. Increasing this value too much and HDBSCAN will have a hard time clustering the high-dimensional embeddings. If you want to increase this value, I would advise setting using a metric for HDBSCAN that works well in high dimensional data.\n",
    "3. **metric**: refers to the method used to compute the distances in high dimensional space. The default is cosine as we are dealing with high dimensional data.\n",
    "\n",
    "\n",
    "#### HDBSCAN & Its Hyperparameters\n",
    "After reducing the embeddings with UMAP, we use HDBSCAN to cluster our documents into clusters of similar documents. Similar to UMAP, HDBSCAN has many parameters that could be tweaked to improve the cluster's quality.\n",
    "1. **min_cluster_size**: arguably the most important parameter in HDBSCAN.  It controls the minimum size of a cluster and thereby the number of clusters that will be generated. It is set to 10 as a default. Increasing this value results in fewer clusters but of larger size whereas decreasing this value results in more micro clusters being generated. Typically, I would advise increasing this value rather than decreasing it.\n",
    "2. **min_samples**: is automatically set to min_cluster_size and controls the number of outliers generated. Setting this value significantly lower than min_cluster_size might help you reduce the amount of noise you will get. Do note that outliers are to be expected and forcing the output to have no outliers may not properly represent the data.\n",
    "3. **metric**:  like with HDBSCAN is used to calculate the distances. Here, we went with euclidean as, after reducing the dimensionality, we have low dimensional data and not much optimization is necessary. However, if you increase n_components in UMAP, then it would be advised to look into metrics that work with high dimensional data.\n",
    "\n",
    "#### Vectorizers \n",
    "In topic modeling, the quality of the topic representations is key for interpreting the topics, communicating results, and understanding patterns. It is of utmost importance to make sure that the topic representations fit with your use case.\n",
    "In practice, there is not one correct way of creating topic representations. Some use cases might opt for higher n-grams, whereas others might focus more on single words without any stop words. \n",
    "1. **ngram_range**: allows us to decide how many tokens each entity is in a topic representation. For example, we have words like game and team with a length of 1 in a topic but it would also make sense to have words like hockey league with a length of 2\n",
    "2. **stop_words**: In some of the topics, we can see stop words appearing like he or the.\n",
    "Stop words are something we typically want to prevent in our topic representations as they do not give additional information to the topic\n",
    "3. **min_df**:  typically an integer representing how frequent a word must be before being added to our representation. You can imagine that if we have a million documents and a certain word only appears a single time across all of them, then it would be highly unlikely to be representative of a topic. Typically, the c-TF-IDF calculation removes that word from the topic representation but when you have millions of documents, that will also lead to a very large topic-term matrix.\n",
    "4. **max_features**: A parameter similar to min_df is max_features which allows you to select the top n most frequent words to be used in the topic representation. Setting this, for example, to 10_000 creates a topic-term matrix with 10_000 terms. This helps you control the size of the topic-term matrix directly without having to fiddle around with the min_df parameter:\n",
    "5. **tokenizer**: The default tokenizer in the CountVectorizer works well for western languages but fails to tokenize some non-western languages, like Chinese. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Batches: 100%|██████████| 1137/1137 [09:29<00:00,  2.00it/s]\n",
      "2024-01-09 20:53:15,174 - BERTopic - Transformed documents to Embeddings\n",
      "2024-01-09 20:54:30,962 - BERTopic - Reduced dimensionality\n",
      "2024-01-09 20:54:48,333 - BERTopic - Clustered reduced embeddings\n",
      "2024-01-09 20:55:19,552 - BERTopic - Reduced number of topics from 69 to 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19616470366850333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import TextGeneration\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from transformers import pipeline, AutoModel\n",
    "from bertopic.representation import OpenAI\n",
    "from hdbscan import HDBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# my_model = BERTopic.load(\"/content/drive/MyDrive/Colab_Notebooks/Data/twitter_bert_model\")\n",
    "\n",
    "# Build the pipeline with the current parameter settings\n",
    "stopwords_list      = list(stopwords.words('english')) + ['http', 'https', 'amp', 'com', 'gtgtgt', 'please', 'send', 'dm']\n",
    "vectorizer_model    = CountVectorizer(min_df=5,\n",
    "                                      ngram_range=(1,2),\n",
    "                                      stop_words=stopwords_list)\n",
    "embedding_model     = AutoModel.from_pretrained('roberta-base')\n",
    "umap_model          = UMAP(n_neighbors= 15,\n",
    "                           n_components= 7,\n",
    "                           min_dist= 0.1,\n",
    "                           random_state= 42)\n",
    "hdbscan_model       = HDBSCAN(min_cluster_size= 100,\n",
    "                              min_samples= 40,\n",
    "                              gen_min_span_tree=True,\n",
    "                              prediction_data=True)\n",
    "ctfidf = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model = representation_model,\n",
    "    ctfidf_model=ctfidf,\n",
    "    top_n_words=10,\n",
    "    min_topic_size=100,\n",
    "    language='english',\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    nr_topics = 50\n",
    "    )\n",
    "\n",
    "# Fit the BERTopic model\n",
    "topics, probs = model.fit_transform(df_tweets_preprocessed['text_preprocessed'])\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(probs, hdbscan_model.labels_)\n",
    "\n",
    "print(silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kushl\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# # Method 3 - pickle\n",
    "model.save(\"../model/localBERT\", serialization=\"pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
