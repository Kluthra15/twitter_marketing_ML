{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "The following code will remove emoticons, hyperlinks, whitepsaces such as new lines and indentations.\n",
    "It will also tokenize text into words, remove slang words, and visualize the text pre-processed data to show patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnicode(text):\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
    "    return text\n",
    "\n",
    "def replaceURL(text):\n",
    "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEmoticons(text):\n",
    "    \"\"\" Removes emoticons from text \"\"\"\n",
    "    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
    "    return text\n",
    "\n",
    "def countEmoticons(text):\n",
    "    \"\"\" Input: a text, Output: how many emoticons \"\"\"\n",
    "    return len(re.findall(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "\"\"\" Tokenizes a text to its words, removes and replaces some of them \"\"\"    \n",
    "finalTokens = [] # all tokens\n",
    "stoplist = stopwords.words('english')\n",
    "my_stopwords = \"multiexclamation multiquestion multistop url atuser st rd nd th am pm\" # my extra stopwords\n",
    "stoplist = stoplist + my_stopwords.split()\n",
    "allowedWordTypes = [\"J\",\"R\",\"V\",\"N\"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging\n",
    "\n",
    "# Create a DataFrame to store the preprocessed text and tweet ID\n",
    "preprocessed_data = pd.DataFrame(columns=['tweet_id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, wordCountBefore, tweet_id):\n",
    "    totalAdjectives = 0\n",
    "    totalAdverbs = 0\n",
    "    totalVerbs = 0\n",
    "    onlyOneSentenceTokens = []  # tokens of one sentence each time\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)  # Technique 7: remove punctuation\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    tagged = nltk.pos_tag(tokens)  # Technique 13: part of speech tagging\n",
    "    for w in tagged:\n",
    "        if w[1][0] in allowedWordTypes and w[0] not in stoplist:\n",
    "            onlyOneSentenceTokens.append(w[0])\n",
    "            finalTokens.append(w[0])\n",
    "\n",
    "    onlyOneSentence = \" \".join(onlyOneSentenceTokens)\n",
    "\n",
    "    # Store the preprocessed text and tweet ID in the DataFrame\n",
    "    preprocessed_data.loc[len(preprocessed_data)] = [tweet_id, onlyOneSentence]\n",
    "\n",
    "    return finalTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Retrieve all tweets from the database\n",
    "query = \"SELECT DISTINCT * FROM tweets\"\n",
    "df_tweets = pd.read_sql_query(query, conn)\n",
    "#Tweet_id is column 0, text is column 3\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "totalSentences = 0\n",
    "totalEmoticons = 0\n",
    "totalSlangs = 0\n",
    "totalSlangsFound = []\n",
    "totalElongated = 0\n",
    "totalMultiExclamationMarks = 0\n",
    "totalMultiQuestionMarks = 0\n",
    "totalMultiStopMarks = 0\n",
    "totalAllCaps = 0\n",
    "\n",
    "# Iterate over each row in df_tweets\n",
    "for index, row in df_tweets.iterrows():\n",
    "    totalSentences += 1\n",
    "    feat = []\n",
    "\n",
    "    tweet_id = row['tweet_id']\n",
    "    text     = removeUnicode(row['text'])  # Technique 0\n",
    "    text     = replaceURL(row['text'])\n",
    "\n",
    "    wordCountBefore = len(re.findall(r'\\w+', text))  # word count of one sentence before preprocess\n",
    "\n",
    "    emoticons = countEmoticons(text)  # how many emoticons in this sentence\n",
    "    totalEmoticons += emoticons\n",
    "\n",
    "    text = removeEmoticons(text)  # removes emoticons from text\n",
    "    tokens = tokenize(text, wordCountBefore, tweet_id)  \n",
    "\n",
    "    # print(\"Processed tweet:\", tweet_id)  # Print the tweet ID after processing\n",
    "\n",
    "# View the resulting preprocessed data\n",
    "print(preprocessed_data.head())\n",
    "    \n",
    "print(\"Total sentences: \",                          totalSentences,\"\\n\")\n",
    "print(\"Total Words before preprocess: \",            len(re.findall(r'\\w+', ' '.join(df_tweets['text']))))\n",
    "print(\"Total Distinct Tokens before preprocess: \",  len(set(re.findall(r'\\w+', ' '.join(df_tweets['text'])))))\n",
    "print(\"Average word/sentence before preprocess: \",  len(re.findall(r'\\w+', ' '.join(df_tweets['text']))) / totalSentences, \"\\n\")\n",
    "print(\"Total Words after preprocess: \",             len(tokens))\n",
    "print(\"Total Distinct Tokens after preprocess: \",   len(set(tokens)))\n",
    "print(\"Average word/sentence after preprocess: \",   len(tokens)/totalSentences,\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Total run time: \",                           time() - t0,\" seconds\\n\")\n",
    "\n",
    "print(\"Total emoticons: \",                          totalEmoticons,\"\\n\")\n",
    "print(\"Total slangs: \",                             totalSlangs,\"\\n\")\n",
    "# commonSlangs = nltk.FreqDist(totalSlangsFound)\n",
    "# for (word, count) in commonSlangs.most_common(20): # most common slangs across all texts\n",
    "#     print(word,\"\\t\",count)\n",
    "# commonSlangs.plot(20, cumulative=False) # plot most common slangs\n",
    "\n",
    "print(\"Total elongated words: \",                    totalElongated,\"\\n\")\n",
    "print(\"Total multi exclamation marks: \",            totalMultiExclamationMarks)\n",
    "print(\"Total multi question marks: \",               totalMultiQuestionMarks)\n",
    "print(\"Total multi stop marks: \",                   totalMultiStopMarks,\"\\n\")\n",
    "print(\"Total all capitalized words: \",              totalAllCaps,\"\\n\")\n",
    "\n",
    "#print(tokens)\n",
    "commonWords = nltk.FreqDist(tokens)\n",
    "print(\"Most common words \")\n",
    "print(\"Word\\tCount\")\n",
    "for (word, count) in commonWords.most_common(100): # most common words across all texts\n",
    "    print(word,\"\\t\",count)\n",
    "\n",
    "# Create a larger plot with adjusted dimensions\n",
    "plt.figure(figsize=(20, 8))  # Adjust the width and height as needed\n",
    "commonWords.plot(100, cumulative=False) # plot most common words\n",
    "\n",
    "\n",
    "bgm = nltk.collocations.BigramAssocMeasures()\n",
    "tgm = nltk.collocations.TrigramAssocMeasures()\n",
    "bgm_finder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "tgm_finder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n",
    "bgm_finder.apply_freq_filter(5) # bigrams that occur at least 5 times\n",
    "print(\"Most common collocations (bigrams)\")\n",
    "print(bgm_finder.nbest(bgm.pmi, 50)) # top 50 bigram collocations\n",
    "tgm_finder.apply_freq_filter(5) # trigrams that occur at least 5 times\n",
    "print(\"Most common collocations (trigrams)\")\n",
    "print(tgm_finder.nbest(tgm.pmi, 20)) # top 20 trigrams collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the preprocessed_data DataFrame with df_tweets based on tweet_id\n",
    "df_tweets_preprocessed = df_tweets.merge(preprocessed_data, on='tweet_id', how='left', suffixes=('_original', '_preprocessed'))\n",
    "\n",
    "# Deduplicate the merged DataFrame based on the 'tweet_id' column\n",
    "df_tweets_preprocessed = df_tweets_preprocessed.drop_duplicates(subset=['tweet_id', 'text_preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_preprocessed.info()\n",
    "df_tweets_preprocessed.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
