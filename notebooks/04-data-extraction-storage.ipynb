{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(query, max_results):\n",
    "  \"\"\"\n",
    "    Fetches recent tweets from the Twitter API based on a given query.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The query string to search for tweets.\n",
    "        max_results (int): The maximum number of tweets to fetch.\n",
    "\n",
    "    Returns:\n",
    "        List of tweets (objects): A list of fetched tweets matching the query.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error while fetching tweets.\n",
    "  \"\"\"\n",
    "  expansions    = ['author_id','in_reply_to_user_id','geo.place_id','entities.mentions.username','referenced_tweets.id','referenced_tweets.id.author_id']\n",
    "  tweet_fields  = ['id','text','author_id','attachments','context_annotations','created_at','entities','lang','geo','public_metrics']\n",
    "  user_fields   = ['id','name','username','created_at','description','entities','location','public_metrics','verified']\n",
    "  place_fields  = ['full_name','id','country','country_code','geo','name','place_type']\n",
    "  try:\n",
    "    # call twitter api to fetch tweets\n",
    "    fetched_tweets = tweepy.Paginator(client.search_recent_tweets, query=query,\n",
    "      expansions        =expansions,\n",
    "      tweet_fields      =tweet_fields,\n",
    "      place_fields      =place_fields,\n",
    "      user_fields       =user_fields,   \n",
    "      max_results       =max_results\n",
    "    ).flatten()\n",
    "    \n",
    "    return fetched_tweets\n",
    "    \n",
    "\n",
    "  except Exception as e:\n",
    "    print(\"Error getting tweets\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df = pd.read_csv('twitter-context-annotations/files/evergreen-context-entities-20220601.csv')\n",
    "import itertools\n",
    "def automate_domain_filter(df, start_id, end_id, chunk_size, domain_chunk_count):\n",
    "    \"\"\"\n",
    "    Automates the process of creating chunks of query strings for domain filtering based on the provided DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the domain information.\n",
    "        start_id (int): The starting domain ID.\n",
    "        end_id (int): The ending domain ID.\n",
    "        chunk_size (int): The maximum size of each chunk in characters.\n",
    "        domain_chunk_count (dict): A dictionary specifying the number of chunks for each domain ID.\n",
    "\n",
    "    Returns:\n",
    "        List of chunks (list): A list of query string chunks.\n",
    "    \"\"\"\n",
    "    chunks_list = []\n",
    "    for i in range(start_id, end_id+1):\n",
    "        context_list = []\n",
    "        mask = df['domains'].str.contains('^{}$'.format(i))\n",
    "        filtered_df = df[mask]\n",
    "        for index, row in filtered_df.iterrows():\n",
    "            domain_id = row['domains']\n",
    "            entity_id = row['entity_id']\n",
    "            entity_name = row['entity_name']   \n",
    "            # construct the query string\n",
    "            context = f'context:{domain_id}.{entity_id}'\n",
    "            context_list.append(context)\n",
    "            context_query = ' OR '.join(context_list)\n",
    "        code = context_query\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        counter = 0\n",
    "        while start < len(code) and counter < domain_chunk_count[i]:\n",
    "            end = start + chunk_size\n",
    "            if end >= len(code):\n",
    "                end = len(code)\n",
    "            end = code.rfind(\" OR \", start, end)\n",
    "            if end == -1:\n",
    "                end = start + chunk_size\n",
    "            chunk = code[start:end]\n",
    "            if chunk.startswith(\" OR \"):\n",
    "                chunk = chunk[4:]\n",
    "            chunks.append(chunk)\n",
    "            start = end\n",
    "            counter += 1\n",
    "        chunks_list.append(chunks)\n",
    "    return list(itertools.chain.from_iterable(chunks_list))\n",
    "\n",
    "chunk_size = 350\n",
    "domain_chunk_count = {45: 1, 46: 6, 47: 276, 48: 69}\n",
    "chunks_list = automate_domain_filter(domain_df, 45, 48, chunk_size, domain_chunk_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rule(chunk, hash_include=True):\n",
    "    \"\"\"\n",
    "    Creates a query string for filtering tweets based on the provided chunk.\n",
    "\n",
    "    Parameters:\n",
    "        chunk (str): The chunk representing a portion of the query string.\n",
    "        hash_include (bool): Flag indicating whether to include hashtag-related filter rules. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        query (str): The constructed query string.\n",
    "    \"\"\"\n",
    "    text_list       = '(#ad OR #sponsored OR #promoted OR \"Learn More\" OR \"Shop Now\")'\n",
    "    lang            = '(lang:en)'\n",
    "    rt              = '(-is:retweet) (-\"RT\")' \n",
    "    domain            = chunk\n",
    "    mention         = 'has:mentions'\n",
    "    if hash_include == True:\n",
    "        query           = text_list + ' ' + lang + ' ' + rt + ' ' + mention + ' ' + '(' + domain + ')'\n",
    "    else: \n",
    "        query           = lang + ' ' + rt + ' ' + mention + ' ' + '(' + domain + ')'\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "processed_tweets = 0\n",
    "total_tweets = 0\n",
    "\n",
    "tweet_data = []  # List to store tweet data\n",
    "\n",
    "for chunk in chunks_list:\n",
    "    print(f'Chunk: {chunk}')\n",
    "    \n",
    "    query = filter_rule(chunk=chunk,hash_include=True)\n",
    "    paginator = get_tweets(query=query, max_results=100)\n",
    "\n",
    "    if paginator is None:\n",
    "        print('Error: Paginator is None. Skipping chunk.')\n",
    "        continue\n",
    "    \n",
    "    for tweet in paginator:\n",
    "        tweet_info = {\n",
    "                'tweet_id': tweet.id,\n",
    "                'author_id': tweet.author_id,\n",
    "                'created_at': tweet.created_at,\n",
    "                'text': tweet.text,\n",
    "                'tweet_metrics': json.dumps(tweet.public_metrics),\n",
    "                'entities': json.dumps(tweet.entities),\n",
    "                'context': json.dumps(tweet.context_annotations),\n",
    "                'place_id': json.dumps(tweet.geo) if tweet.geo else None\n",
    "            }\n",
    "            \n",
    "        tweet_data.append(tweet_info)\n",
    "        processed_tweets += 1\n",
    "    \n",
    "    \n",
    "    print(f'Finished processing chunk: {chunk}')\n",
    "    print(f'Progress: {processed_tweets} tweets processed.')\n",
    "    time.sleep(3)  # Pause for 5 minutes between chunks to avoid hitting rate limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tweet_data list to a DataFrame\n",
    "df = pd.DataFrame(tweet_data)\n",
    "\n",
    "# Sort the DataFrame by 'tweet_id' in descending order\n",
    "df.sort_values('tweet_id', ascending=True, inplace=True)\n",
    "\n",
    "# Drop duplicate rows based on 'tweet_id' column, keeping the last occurrence\n",
    "dedup_df = df.drop_duplicates(subset='tweet_id', keep='last', inplace=False).reset_index(drop=True, inplace=False)\n",
    "\n",
    "print(dedup_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets_to_database(df, conn, c):\n",
    "    \"\"\"\n",
    "    Processes tweets from a DataFrame and inserts or updates them in a SQLite database.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing tweet data.\n",
    "        conn (sqlite3.Connection): SQLite database connection.\n",
    "        c (sqlite3.Cursor): SQLite database cursor.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of tweets processed.\n",
    "\n",
    "    \"\"\"\n",
    "    processed_tweets = 0\n",
    "\n",
    "    for index, tweet in df.iterrows():\n",
    "        try:\n",
    "            created_at = tweet['created_at'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # Check if tweet with the same tweet_id already exists in the database\n",
    "            c.execute('SELECT tweet_id FROM tweets WHERE tweet_id=?', (tweet['tweet_id'],))\n",
    "            existing_tweet_id = c.fetchone()\n",
    "\n",
    "            if existing_tweet_id is None:\n",
    "                # Tweet doesn't exist in the database, insert it\n",
    "                c.execute('''INSERT INTO tweets \n",
    "                             (tweet_id, author_id, created_at, text, tweet_metrics, entities, context, place_id) \n",
    "                             VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',\n",
    "                          (tweet['tweet_id'], tweet['author_id'], created_at, tweet['text'],\n",
    "                           tweet['tweet_metrics'], tweet['entities'], tweet['context'], tweet['place_id']))\n",
    "                print(\"New Tweet Appended\")\n",
    "            else:\n",
    "                # Tweet already exists, update tweet_metrics\n",
    "                c.execute('''UPDATE tweets \n",
    "                             SET tweet_metrics = ? \n",
    "                             WHERE tweet_id = ?''',\n",
    "                          (tweet['tweet_metrics'], tweet['tweet_id']))\n",
    "                print(\"Tweet Already Exists, Updating Tweet Metrics\")\n",
    "            processed_tweets += 1\n",
    "\n",
    "            print(f'Progress: {processed_tweets} tweets processed.')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting row: {tweet}\")\n",
    "            print(f\"Error message: {e}\")\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "\n",
    "    return processed_tweets\n",
    "\n",
    "process_tweets_to_database(df=dedup_df, conn=conn, c=c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique author IDs from the tweets table\n",
    "c.execute(\"SELECT DISTINCT author_id FROM tweets\")\n",
    "author_ids = [str(row[0]) for row in c.fetchall()]\n",
    "\n",
    "user_data = []\n",
    "batch_size = 100\n",
    "n = 0\n",
    "\n",
    "# Iterate over batches of author IDs\n",
    "for i in range(0, len(author_ids), batch_size):  \n",
    "    # Get a batch of user IDs\n",
    "    user_ids_batch = author_ids[i:i+batch_size]    \n",
    "    try:\n",
    "        users = t.users.lookup(user_id=\",\".join(user_ids_batch))\n",
    "        \n",
    "    # Insert or update the user data in the database\n",
    "        for user in users:\n",
    "            # Check if author already exists in the database\n",
    "            c.execute(\"SELECT author_id FROM users WHERE author_id=?\",  (user['id_str'],))\n",
    "            existing_author_id = c.fetchone()\n",
    "\n",
    "            if existing_author_id is None:\n",
    "                # Author doesn't exist in the database, insert a new row\n",
    "                author_created = user['created_at']\n",
    "                c.execute('''INSERT INTO users (author_id, username, verified, bio, author_created, author_location, \n",
    "                            followers_count, following_count, tweet_count, entities)\n",
    "                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',\n",
    "                            (user['id_str'], user['screen_name'], user['verified'], user['description'], author_created,\n",
    "                            user['location'], user['followers_count'], user['friends_count'], user['statuses_count'],\n",
    "                           json.dumps(user['entities'])))\n",
    "                print(f\"Stored author: {user['name']} (@{user['screen_name']}), id={user['id_str']}\")\n",
    "            else:\n",
    "                # Author already exists in the database, update the existing row\n",
    "                author_created = user['created_at']\n",
    "                c.execute('''UPDATE users SET username=?, verified=?, bio=?, author_created=?, author_location=?, \n",
    "                            followers_count=?, following_count=?, tweet_count=?, entities=?\n",
    "                            WHERE author_id=?''',\n",
    "                            (user['screen_name'], user['verified'], user['description'], author_created,\n",
    "                            user['location'], user['followers_count'], user['friends_count'], user['statuses_count'],\n",
    "                           json.dumps(user['entities']), user['id_str']))\n",
    "                print(f\"Updated author: {user['name']} (@{user['screen_name']}), id={user['id_str']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving user data: {str(e)}\")\n",
    "    time.sleep(10)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute(\"PRAGMA table_info(tweets)\")\n",
    "columns = c.fetchall()\n",
    "\n",
    "for column in columns:\n",
    "    print(column[1], \"-\", column[2])\n",
    "\n",
    "c.execute(\"SELECT COUNT(DISTINCT tweet_id) FROM tweets\")\n",
    "row_count = c.fetchone()[0]\n",
    "print(f\"Number of rows in 'tweets' table: {row_count}\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "c.execute(\"PRAGMA table_info(users)\")\n",
    "columns = c.fetchall()\n",
    "\n",
    "for column in columns:\n",
    "    print(column[1], \"-\", column[2])\n",
    "\n",
    "c.execute(\"SELECT COUNT(DISTINCT author_id) FROM users\")\n",
    "row_count = c.fetchone()[0]\n",
    "print(f\"Number of rows in 'users' table: {row_count}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
